2020-11-03 19:22:29.389402: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-11-03 19:22:29.482882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-03 19:22:29.483443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Quadro P5000 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:0b:00.0
totalMemory: 15.90GiB freeMemory: 15.60GiB
2020-11-03 19:22:29.483464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-11-03 19:22:29.772464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-03 19:22:29.772498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-11-03 19:22:29.772504: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-11-03 19:22:29.772601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15118 MB memory) -> physical GPU (device: 0, name: Quadro P5000, pci bus id: 0000:0b:00.0, compute capability: 6.1)
X_train shape: (9607, 1, 32, 231)
X_testshape: (4806, 1, 32, 231)
9607 train samples
4806 test samples
chans: 32 samples: 231
Train on 9607 samples, validate on 4802 samples
Epoch 1/200

Epoch 00001: val_loss improved from inf to 0.58304, saving model to /tmp/checkpoint.h5
 - 4s - loss: 0.6779 - acc: 0.8980 - val_loss: 0.5830 - val_acc: 0.9336
Epoch 2/200

Epoch 00002: val_loss improved from 0.58304 to 0.53266, saving model to /tmp/checkpoint.h5
 - 3s - loss: 0.6601 - acc: 0.9280 - val_loss: 0.5327 - val_acc: 0.9119
Epoch 3/200

Epoch 00003: val_loss did not improve from 0.53266
 - 3s - loss: 0.6318 - acc: 0.8543 - val_loss: 0.5414 - val_acc: 0.8080
Epoch 4/200

Epoch 00004: val_loss improved from 0.53266 to 0.50791, saving model to /tmp/checkpoint.h5
 - 3s - loss: 0.6169 - acc: 0.8420 - val_loss: 0.5079 - val_acc: 0.8484
Epoch 5/200

Epoch 00005: val_loss improved from 0.50791 to 0.47704, saving model to /tmp/checkpoint.h5
 - 3s - loss: 0.6045 - acc: 0.8247 - val_loss: 0.4770 - val_acc: 0.8599
Epoch 6/200

Epoch 00006: val_loss did not improve from 0.47704
 - 3s - loss: 0.5985 - acc: 0.8237 - val_loss: 0.5240 - val_acc: 0.8092
Epoch 7/200

Epoch 00007: val_loss did not improve from 0.47704
 - 3s - loss: 0.5908 - acc: 0.8346 - val_loss: 0.5031 - val_acc: 0.8315
Epoch 8/200

Epoch 00008: val_loss did not improve from 0.47704
 - 3s - loss: 0.5830 - acc: 0.8235 - val_loss: 0.5156 - val_acc: 0.7970
Epoch 9/200

Epoch 00009: val_loss did not improve from 0.47704
 - 3s - loss: 0.5723 - acc: 0.8152 - val_loss: 0.4828 - val_acc: 0.8290
Epoch 10/200

Epoch 00010: val_loss did not improve from 0.47704
 - 3s - loss: 0.5719 - acc: 0.8370 - val_loss: 0.5313 - val_acc: 0.7718
Epoch 11/200

Epoch 00011: val_loss did not improve from 0.47704
 - 3s - loss: 0.5673 - acc: 0.8248 - val_loss: 0.4939 - val_acc: 0.8051
Epoch 12/200

Epoch 00012: val_loss did not improve from 0.47704
 - 3s - loss: 0.5566 - acc: 0.8175 - val_loss: 0.4923 - val_acc: 0.8182
Epoch 13/200

Epoch 00013: val_loss did not improve from 0.47704
 - 3s - loss: 0.5564 - acc: 0.8174 - val_loss: 0.5047 - val_acc: 0.8063
Epoch 14/200

Epoch 00014: val_loss did not improve from 0.47704
 - 3s - loss: 0.5488 - acc: 0.8011 - val_loss: 0.5033 - val_acc: 0.7943
Epoch 15/200

Epoch 00015: val_loss improved from 0.47704 to 0.45243, saving model to /tmp/checkpoint.h5
 - 3s - loss: 0.5425 - acc: 0.8186 - val_loss: 0.4524 - val_acc: 0.8488
Epoch 16/200

Epoch 00016: val_loss did not improve from 0.45243
 - 3s - loss: 0.5452 - acc: 0.8069 - val_loss: 0.5012 - val_acc: 0.7982
Epoch 17/200

Epoch 00017: val_loss did not improve from 0.45243
 - 3s - loss: 0.5440 - acc: 0.8126 - val_loss: 0.5136 - val_acc: 0.7557
Epoch 18/200

Epoch 00018: val_loss did not improve from 0.45243
 - 3s - loss: 0.5401 - acc: 0.8042 - val_loss: 0.4920 - val_acc: 0.8084
Epoch 19/200

Epoch 00019: val_loss did not improve from 0.45243
 - 3s - loss: 0.5336 - acc: 0.8169 - val_loss: 0.4826 - val_acc: 0.8157
Epoch 20/200

Epoch 00020: val_loss did not improve from 0.45243
 - 3s - loss: 0.5252 - acc: 0.8045 - val_loss: 0.4710 - val_acc: 0.8138
Epoch 21/200

Epoch 00021: val_loss did not improve from 0.45243
 - 3s - loss: 0.5317 - acc: 0.8026 - val_loss: 0.4643 - val_acc: 0.8265
Epoch 22/200

Epoch 00022: val_loss improved from 0.45243 to 0.43063, saving model to /tmp/checkpoint.h5
 - 3s - loss: 0.5259 - acc: 0.8088 - val_loss: 0.4306 - val_acc: 0.8442
Epoch 23/200

Epoch 00023: val_loss did not improve from 0.43063
 - 3s - loss: 0.5251 - acc: 0.8163 - val_loss: 0.5160 - val_acc: 0.7836
Epoch 24/200

Epoch 00024: val_loss did not improve from 0.43063
 - 3s - loss: 0.5255 - acc: 0.7914 - val_loss: 0.5314 - val_acc: 0.7647
Epoch 25/200

Epoch 00025: val_loss did not improve from 0.43063
 - 3s - loss: 0.5188 - acc: 0.8173 - val_loss: 0.5640 - val_acc: 0.6976
Epoch 26/200

Epoch 00026: val_loss did not improve from 0.43063
 - 3s - loss: 0.5200 - acc: 0.8033 - val_loss: 0.5118 - val_acc: 0.7768
Epoch 27/200

Epoch 00027: val_loss did not improve from 0.43063
 - 3s - loss: 0.5261 - acc: 0.8096 - val_loss: 0.5279 - val_acc: 0.7378
Epoch 28/200

Epoch 00028: val_loss did not improve from 0.43063
 - 3s - loss: 0.5167 - acc: 0.8146 - val_loss: 0.4819 - val_acc: 0.8178
Epoch 29/200

Epoch 00029: val_loss did not improve from 0.43063
 - 3s - loss: 0.5191 - acc: 0.8034 - val_loss: 0.4681 - val_acc: 0.8311
Epoch 30/200

Epoch 00030: val_loss did not improve from 0.43063
 - 3s - loss: 0.5098 - acc: 0.8172 - val_loss: 0.5670 - val_acc: 0.6822
Epoch 31/200

Epoch 00031: val_loss did not improve from 0.43063
 - 3s - loss: 0.5101 - acc: 0.8171 - val_loss: 0.5233 - val_acc: 0.7745
Epoch 32/200

Epoch 00032: val_loss did not improve from 0.43063
 - 3s - loss: 0.5148 - acc: 0.8199 - val_loss: 0.5338 - val_acc: 0.7605
Epoch 33/200

Epoch 00033: val_loss did not improve from 0.43063
 - 3s - loss: 0.5080 - acc: 0.8201 - val_loss: 0.5166 - val_acc: 0.7713
Epoch 34/200

Epoch 00034: val_loss did not improve from 0.43063
 - 3s - loss: 0.5005 - acc: 0.8085 - val_loss: 0.5117 - val_acc: 0.7809
Epoch 35/200

Epoch 00035: val_loss did not improve from 0.43063
 - 3s - loss: 0.5036 - acc: 0.8109 - val_loss: 0.4978 - val_acc: 0.8015
Epoch 36/200

Epoch 00036: val_loss did not improve from 0.43063
 - 3s - loss: 0.5064 - acc: 0.8134 - val_loss: 0.4981 - val_acc: 0.8009
Epoch 37/200

Epoch 00037: val_loss did not improve from 0.43063
 - 3s - loss: 0.5039 - acc: 0.8006 - val_loss: 0.4965 - val_acc: 0.7930
Epoch 38/200

Epoch 00038: val_loss did not improve from 0.43063
 - 3s - loss: 0.5004 - acc: 0.8135 - val_loss: 0.5291 - val_acc: 0.7461
Epoch 39/200

Epoch 00039: val_loss did not improve from 0.43063
 - 3s - loss: 0.4990 - acc: 0.8209 - val_loss: 0.5468 - val_acc: 0.7295
Epoch 40/200

Epoch 00040: val_loss did not improve from 0.43063
 - 3s - loss: 0.4903 - acc: 0.8253 - val_loss: 0.5013 - val_acc: 0.7913
Epoch 41/200

Epoch 00041: val_loss did not improve from 0.43063
 - 3s - loss: 0.5102 - acc: 0.8085 - val_loss: 0.5548 - val_acc: 0.7074
Epoch 42/200

Epoch 00042: val_loss did not improve from 0.43063
 - 3s - loss: 0.4997 - acc: 0.8087 - val_loss: 0.5267 - val_acc: 0.7436
Epoch 43/200

Epoch 00043: val_loss did not improve from 0.43063
 - 3s - loss: 0.4962 - acc: 0.8129 - val_loss: 0.5307 - val_acc: 0.7503
Epoch 44/200

Epoch 00044: val_loss did not improve from 0.43063
 - 3s - loss: 0.4802 - acc: 0.8259 - val_loss: 0.5185 - val_acc: 0.7570
Epoch 45/200

Epoch 00045: val_loss did not improve from 0.43063
 - 3s - loss: 0.4864 - acc: 0.8208 - val_loss: 0.5110 - val_acc: 0.7428
Epoch 46/200

Epoch 00046: val_loss did not improve from 0.43063
 - 3s - loss: 0.4961 - acc: 0.8189 - val_loss: 0.5588 - val_acc: 0.7022
Epoch 47/200

Epoch 00047: val_loss did not improve from 0.43063
 - 3s - loss: 0.4945 - acc: 0.8074 - val_loss: 0.4771 - val_acc: 0.8419
Epoch 48/200

Epoch 00048: val_loss did not improve from 0.43063
 - 3s - loss: 0.4884 - acc: 0.8165 - val_loss: 0.4469 - val_acc: 0.8526
Epoch 49/200

Epoch 00049: val_loss did not improve from 0.43063
 - 3s - loss: 0.4865 - acc: 0.8108 - val_loss: 0.4809 - val_acc: 0.8132
Epoch 50/200

Epoch 00050: val_loss did not improve from 0.43063
 - 3s - loss: 0.4959 - acc: 0.8201 - val_loss: 0.4780 - val_acc: 0.8217
Epoch 51/200

Epoch 00051: val_loss did not improve from 0.43063
 - 3s - loss: 0.4722 - acc: 0.8179 - val_loss: 0.4595 - val_acc: 0.8440
Epoch 52/200

Epoch 00052: val_loss did not improve from 0.43063
 - 3s - loss: 0.4803 - acc: 0.8169 - val_loss: 0.4719 - val_acc: 0.8111
Epoch 53/200

Epoch 00053: val_loss did not improve from 0.43063
 - 3s - loss: 0.4863 - acc: 0.8273 - val_loss: 0.4924 - val_acc: 0.8215
Epoch 54/200

Epoch 00054: val_loss did not improve from 0.43063
 - 3s - loss: 0.4883 - acc: 0.8205 - val_loss: 0.4791 - val_acc: 0.8209
Epoch 55/200

Epoch 00055: val_loss did not improve from 0.43063
 - 3s - loss: 0.4750 - acc: 0.8197 - val_loss: 0.4839 - val_acc: 0.8276
Epoch 56/200

Epoch 00056: val_loss did not improve from 0.43063
 - 3s - loss: 0.4718 - acc: 0.8295 - val_loss: 0.4556 - val_acc: 0.8401
Epoch 57/200

Epoch 00057: val_loss did not improve from 0.43063
 - 3s - loss: 0.4773 - acc: 0.8268 - val_loss: 0.5000 - val_acc: 0.8034
Epoch 58/200

Epoch 00058: val_loss improved from 0.43063 to 0.42317, saving model to /tmp/checkpoint.h5
 - 3s - loss: 0.4837 - acc: 0.8187 - val_loss: 0.4232 - val_acc: 0.8703
Epoch 59/200

Epoch 00059: val_loss did not improve from 0.42317
 - 3s - loss: 0.4826 - acc: 0.8154 - val_loss: 0.5357 - val_acc: 0.7691
Epoch 60/200

Epoch 00060: val_loss did not improve from 0.42317
 - 3s - loss: 0.4723 - acc: 0.8154 - val_loss: 0.5181 - val_acc: 0.7857
Epoch 61/200

Epoch 00061: val_loss did not improve from 0.42317
 - 3s - loss: 0.4802 - acc: 0.8186 - val_loss: 0.6560 - val_acc: 0.6033
Epoch 62/200

Epoch 00062: val_loss did not improve from 0.42317
 - 3s - loss: 0.4793 - acc: 0.8182 - val_loss: 0.5197 - val_acc: 0.7909
Epoch 63/200

Epoch 00063: val_loss did not improve from 0.42317
 - 3s - loss: 0.4765 - acc: 0.8173 - val_loss: 0.5121 - val_acc: 0.7936
Epoch 64/200

Epoch 00064: val_loss did not improve from 0.42317
 - 3s - loss: 0.4653 - acc: 0.8191 - val_loss: 0.5698 - val_acc: 0.7028
Epoch 65/200

Epoch 00065: val_loss did not improve from 0.42317
 - 3s - loss: 0.4911 - acc: 0.8111 - val_loss: 0.4947 - val_acc: 0.8203
Epoch 66/200

Epoch 00066: val_loss did not improve from 0.42317
 - 3s - loss: 0.4646 - acc: 0.8202 - val_loss: 0.4893 - val_acc: 0.8334
Epoch 67/200

Epoch 00067: val_loss did not improve from 0.42317
 - 3s - loss: 0.4840 - acc: 0.8106 - val_loss: 0.5603 - val_acc: 0.7630
Epoch 68/200

Epoch 00068: val_loss did not improve from 0.42317
 - 3s - loss: 0.4751 - acc: 0.8151 - val_loss: 0.4992 - val_acc: 0.8151
Epoch 69/200

Epoch 00069: val_loss did not improve from 0.42317
 - 3s - loss: 0.4667 - acc: 0.8213 - val_loss: 0.5159 - val_acc: 0.8090
Epoch 70/200

Epoch 00070: val_loss did not improve from 0.42317
 - 3s - loss: 0.4589 - acc: 0.8255 - val_loss: 0.4893 - val_acc: 0.8244
Epoch 71/200

Epoch 00071: val_loss did not improve from 0.42317
 - 3s - loss: 0.4732 - acc: 0.8277 - val_loss: 0.5388 - val_acc: 0.7795
Epoch 72/200

Epoch 00072: val_loss did not improve from 0.42317
 - 3s - loss: 0.4712 - acc: 0.8159 - val_loss: 0.5009 - val_acc: 0.8249
Epoch 73/200

Epoch 00073: val_loss did not improve from 0.42317
 - 3s - loss: 0.4646 - acc: 0.8225 - val_loss: 0.5434 - val_acc: 0.7730
Epoch 74/200

Epoch 00074: val_loss did not improve from 0.42317
 - 3s - loss: 0.4585 - acc: 0.8364 - val_loss: 0.4735 - val_acc: 0.8336
Epoch 75/200

Epoch 00075: val_loss did not improve from 0.42317
 - 3s - loss: 0.4708 - acc: 0.8382 - val_loss: 0.5866 - val_acc: 0.6880
Epoch 76/200

Epoch 00076: val_loss did not improve from 0.42317
 - 3s - loss: 0.4695 - acc: 0.8154 - val_loss: 0.5044 - val_acc: 0.8207
Epoch 77/200

Epoch 00077: val_loss did not improve from 0.42317
 - 3s - loss: 0.4640 - acc: 0.8326 - val_loss: 0.4973 - val_acc: 0.8313
Epoch 78/200

Epoch 00078: val_loss did not improve from 0.42317
 - 3s - loss: 0.4537 - acc: 0.8263 - val_loss: 0.5614 - val_acc: 0.7499
Epoch 79/200

Epoch 00079: val_loss did not improve from 0.42317
 - 3s - loss: 0.4814 - acc: 0.8238 - val_loss: 0.5025 - val_acc: 0.8172
Epoch 80/200

Epoch 00080: val_loss did not improve from 0.42317
 - 3s - loss: 0.4738 - acc: 0.8207 - val_loss: 0.4862 - val_acc: 0.8267
Epoch 81/200

Epoch 00081: val_loss did not improve from 0.42317
 - 3s - loss: 0.4547 - acc: 0.8269 - val_loss: 0.4601 - val_acc: 0.8394
Epoch 82/200

Epoch 00082: val_loss did not improve from 0.42317
 - 3s - loss: 0.4652 - acc: 0.8224 - val_loss: 0.5356 - val_acc: 0.7697
Epoch 83/200

Epoch 00083: val_loss did not improve from 0.42317
 - 3s - loss: 0.4650 - acc: 0.8237 - val_loss: 0.5480 - val_acc: 0.7574
Epoch 84/200

Epoch 00084: val_loss did not improve from 0.42317
 - 3s - loss: 0.4656 - acc: 0.8305 - val_loss: 0.5220 - val_acc: 0.8011
Epoch 85/200

Epoch 00085: val_loss did not improve from 0.42317
 - 3s - loss: 0.4650 - acc: 0.8245 - val_loss: 0.5165 - val_acc: 0.7965
Epoch 86/200

Epoch 00086: val_loss did not improve from 0.42317
 - 3s - loss: 0.4684 - acc: 0.8235 - val_loss: 0.5403 - val_acc: 0.7813
Epoch 87/200

Epoch 00087: val_loss did not improve from 0.42317
 - 3s - loss: 0.4578 - acc: 0.8323 - val_loss: 0.5166 - val_acc: 0.7940
Epoch 88/200

Epoch 00088: val_loss did not improve from 0.42317
 - 3s - loss: 0.4600 - acc: 0.8258 - val_loss: 0.5028 - val_acc: 0.8126
Epoch 89/200

Epoch 00089: val_loss did not improve from 0.42317
 - 3s - loss: 0.4699 - acc: 0.8230 - val_loss: 0.5041 - val_acc: 0.8107
Epoch 90/200

Epoch 00090: val_loss did not improve from 0.42317
 - 3s - loss: 0.4585 - acc: 0.8286 - val_loss: 0.5207 - val_acc: 0.8101
Epoch 91/200

Epoch 00091: val_loss did not improve from 0.42317
 - 3s - loss: 0.4783 - acc: 0.8163 - val_loss: 0.4977 - val_acc: 0.8203
Epoch 92/200

Epoch 00092: val_loss did not improve from 0.42317
 - 3s - loss: 0.4724 - acc: 0.8220 - val_loss: 0.5297 - val_acc: 0.7884
Epoch 93/200

Epoch 00093: val_loss did not improve from 0.42317
 - 3s - loss: 0.4500 - acc: 0.8312 - val_loss: 0.5132 - val_acc: 0.8076
Epoch 94/200

Epoch 00094: val_loss did not improve from 0.42317
 - 3s - loss: 0.4541 - acc: 0.8349 - val_loss: 0.5247 - val_acc: 0.7859
Epoch 95/200

Epoch 00095: val_loss did not improve from 0.42317
 - 3s - loss: 0.4712 - acc: 0.8256 - val_loss: 0.5061 - val_acc: 0.8136
Epoch 96/200

Epoch 00096: val_loss did not improve from 0.42317
 - 3s - loss: 0.4599 - acc: 0.8214 - val_loss: 0.5047 - val_acc: 0.8140
Epoch 97/200

Epoch 00097: val_loss did not improve from 0.42317
 - 3s - loss: 0.4789 - acc: 0.8309 - val_loss: 0.4993 - val_acc: 0.8122
Epoch 98/200

Epoch 00098: val_loss did not improve from 0.42317
 - 3s - loss: 0.4631 - acc: 0.8292 - val_loss: 0.4871 - val_acc: 0.8411
Epoch 99/200

Epoch 00099: val_loss did not improve from 0.42317
 - 3s - loss: 0.4637 - acc: 0.8300 - val_loss: 0.5403 - val_acc: 0.7834
Epoch 100/200

Epoch 00100: val_loss did not improve from 0.42317
 - 3s - loss: 0.4712 - acc: 0.8150 - val_loss: 0.5220 - val_acc: 0.7938
Epoch 101/200

Epoch 00101: val_loss did not improve from 0.42317
 - 3s - loss: 0.4567 - acc: 0.8288 - val_loss: 0.4820 - val_acc: 0.8224
Epoch 102/200

Epoch 00102: val_loss did not improve from 0.42317
 - 3s - loss: 0.4508 - acc: 0.8261 - val_loss: 0.5336 - val_acc: 0.7872
Epoch 103/200

Epoch 00103: val_loss did not improve from 0.42317
 - 3s - loss: 0.4653 - acc: 0.8147 - val_loss: 0.5397 - val_acc: 0.7793
Epoch 104/200

Epoch 00104: val_loss did not improve from 0.42317
 - 3s - loss: 0.4577 - acc: 0.8233 - val_loss: 0.5298 - val_acc: 0.8076
Epoch 105/200

Epoch 00105: val_loss did not improve from 0.42317
 - 3s - loss: 0.4605 - acc: 0.8317 - val_loss: 0.4720 - val_acc: 0.8451
Epoch 106/200

Epoch 00106: val_loss did not improve from 0.42317
 - 3s - loss: 0.4532 - acc: 0.8275 - val_loss: 0.5736 - val_acc: 0.7432
Epoch 107/200

Epoch 00107: val_loss did not improve from 0.42317
 - 3s - loss: 0.4505 - acc: 0.8183 - val_loss: 0.6066 - val_acc: 0.6916
Epoch 108/200

Epoch 00108: val_loss did not improve from 0.42317
 - 3s - loss: 0.4517 - acc: 0.8218 - val_loss: 0.5230 - val_acc: 0.7932
Epoch 109/200

Epoch 00109: val_loss did not improve from 0.42317
 - 3s - loss: 0.4582 - acc: 0.8235 - val_loss: 0.5125 - val_acc: 0.8128
Epoch 110/200

Epoch 00110: val_loss did not improve from 0.42317
 - 3s - loss: 0.4419 - acc: 0.8386 - val_loss: 0.5288 - val_acc: 0.7822
Epoch 111/200

Epoch 00111: val_loss did not improve from 0.42317
 - 3s - loss: 0.4592 - acc: 0.8353 - val_loss: 0.5585 - val_acc: 0.7559
Epoch 112/200

Epoch 00112: val_loss did not improve from 0.42317
 - 3s - loss: 0.4644 - acc: 0.8276 - val_loss: 0.5496 - val_acc: 0.7630
Epoch 113/200

Epoch 00113: val_loss did not improve from 0.42317
 - 3s - loss: 0.4658 - acc: 0.8201 - val_loss: 0.5026 - val_acc: 0.8103
Epoch 114/200

Epoch 00114: val_loss did not improve from 0.42317
 - 3s - loss: 0.4554 - acc: 0.8208 - val_loss: 0.4957 - val_acc: 0.8286
Epoch 115/200

Epoch 00115: val_loss did not improve from 0.42317
 - 3s - loss: 0.4637 - acc: 0.8241 - val_loss: 0.5036 - val_acc: 0.8180
Epoch 116/200

Epoch 00116: val_loss did not improve from 0.42317
 - 3s - loss: 0.4461 - acc: 0.8351 - val_loss: 0.4561 - val_acc: 0.8621
Epoch 117/200

Epoch 00117: val_loss did not improve from 0.42317
 - 3s - loss: 0.4616 - acc: 0.8328 - val_loss: 0.4980 - val_acc: 0.8011
Epoch 118/200

Epoch 00118: val_loss did not improve from 0.42317
 - 3s - loss: 0.4421 - acc: 0.8298 - val_loss: 0.4755 - val_acc: 0.8421
Epoch 119/200

Epoch 00119: val_loss did not improve from 0.42317
 - 3s - loss: 0.4592 - acc: 0.8265 - val_loss: 0.4869 - val_acc: 0.8369
Epoch 120/200

Epoch 00120: val_loss did not improve from 0.42317
 - 3s - loss: 0.4464 - acc: 0.8399 - val_loss: 0.4991 - val_acc: 0.8269
Epoch 121/200

Epoch 00121: val_loss did not improve from 0.42317
 - 3s - loss: 0.4530 - acc: 0.8349 - val_loss: 0.4815 - val_acc: 0.8244
Epoch 122/200

Epoch 00122: val_loss did not improve from 0.42317
 - 3s - loss: 0.4525 - acc: 0.8221 - val_loss: 0.4482 - val_acc: 0.8563
Epoch 123/200

Epoch 00123: val_loss did not improve from 0.42317
 - 3s - loss: 0.4520 - acc: 0.8336 - val_loss: 0.5067 - val_acc: 0.8209
Epoch 124/200

Epoch 00124: val_loss did not improve from 0.42317
 - 3s - loss: 0.4646 - acc: 0.8229 - val_loss: 0.5435 - val_acc: 0.7895
Epoch 125/200

Epoch 00125: val_loss did not improve from 0.42317
 - 3s - loss: 0.4517 - acc: 0.8258 - val_loss: 0.5028 - val_acc: 0.8349
Epoch 126/200

Epoch 00126: val_loss did not improve from 0.42317
 - 3s - loss: 0.4517 - acc: 0.8295 - val_loss: 0.4879 - val_acc: 0.8161
Epoch 127/200

Epoch 00127: val_loss did not improve from 0.42317
 - 3s - loss: 0.4397 - acc: 0.8303 - val_loss: 0.5439 - val_acc: 0.7863
Epoch 128/200

Epoch 00128: val_loss did not improve from 0.42317
 - 3s - loss: 0.4674 - acc: 0.8317 - val_loss: 0.5737 - val_acc: 0.7380
Epoch 129/200

Epoch 00129: val_loss did not improve from 0.42317
 - 3s - loss: 0.4558 - acc: 0.8260 - val_loss: 0.4838 - val_acc: 0.8380
Epoch 130/200

Epoch 00130: val_loss did not improve from 0.42317
 - 3s - loss: 0.4408 - acc: 0.8402 - val_loss: 0.5423 - val_acc: 0.7763
Epoch 131/200

Epoch 00131: val_loss did not improve from 0.42317
 - 3s - loss: 0.4507 - acc: 0.8268 - val_loss: 0.5156 - val_acc: 0.7990
Epoch 132/200

Epoch 00132: val_loss did not improve from 0.42317
 - 3s - loss: 0.4562 - acc: 0.8378 - val_loss: 0.5571 - val_acc: 0.7478
Epoch 133/200

Epoch 00133: val_loss did not improve from 0.42317
 - 3s - loss: 0.4383 - acc: 0.8344 - val_loss: 0.5518 - val_acc: 0.7713
Epoch 134/200

Epoch 00134: val_loss did not improve from 0.42317
 - 3s - loss: 0.4591 - acc: 0.8274 - val_loss: 0.4824 - val_acc: 0.8322
Epoch 135/200

Epoch 00135: val_loss did not improve from 0.42317
 - 3s - loss: 0.4466 - acc: 0.8267 - val_loss: 0.5236 - val_acc: 0.8001
Epoch 136/200

Epoch 00136: val_loss did not improve from 0.42317
 - 3s - loss: 0.4395 - acc: 0.8329 - val_loss: 0.4833 - val_acc: 0.8490
Epoch 137/200

Epoch 00137: val_loss did not improve from 0.42317
 - 3s - loss: 0.4551 - acc: 0.8293 - val_loss: 0.5343 - val_acc: 0.7672
Epoch 138/200

Epoch 00138: val_loss did not improve from 0.42317
 - 3s - loss: 0.4478 - acc: 0.8255 - val_loss: 0.4924 - val_acc: 0.8174
Epoch 139/200

Epoch 00139: val_loss did not improve from 0.42317
 - 3s - loss: 0.4462 - acc: 0.8284 - val_loss: 0.5084 - val_acc: 0.8261
Epoch 140/200

Epoch 00140: val_loss did not improve from 0.42317
 - 3s - loss: 0.4326 - acc: 0.8300 - val_loss: 0.4984 - val_acc: 0.8249
Epoch 141/200

Epoch 00141: val_loss did not improve from 0.42317
 - 3s - loss: 0.4458 - acc: 0.8387 - val_loss: 0.5081 - val_acc: 0.8309
Epoch 142/200

Epoch 00142: val_loss did not improve from 0.42317
 - 3s - loss: 0.4400 - acc: 0.8334 - val_loss: 0.5901 - val_acc: 0.6935
Epoch 143/200

Epoch 00143: val_loss did not improve from 0.42317
 - 3s - loss: 0.4462 - acc: 0.8437 - val_loss: 0.5300 - val_acc: 0.7859
Epoch 144/200

Epoch 00144: val_loss did not improve from 0.42317
 - 3s - loss: 0.4599 - acc: 0.8271 - val_loss: 0.5112 - val_acc: 0.8040
Epoch 145/200

Epoch 00145: val_loss did not improve from 0.42317
 - 3s - loss: 0.4502 - acc: 0.8203 - val_loss: 0.5838 - val_acc: 0.6962
Epoch 146/200

Epoch 00146: val_loss did not improve from 0.42317
 - 3s - loss: 0.4442 - acc: 0.8220 - val_loss: 0.5489 - val_acc: 0.7761
Epoch 147/200

Epoch 00147: val_loss did not improve from 0.42317
 - 3s - loss: 0.4543 - acc: 0.8345 - val_loss: 0.5590 - val_acc: 0.7384
Epoch 148/200

Epoch 00148: val_loss did not improve from 0.42317
 - 3s - loss: 0.4370 - acc: 0.8412 - val_loss: 0.5008 - val_acc: 0.8359
Epoch 149/200

Epoch 00149: val_loss did not improve from 0.42317
 - 3s - loss: 0.4448 - acc: 0.8358 - val_loss: 0.5032 - val_acc: 0.8319
Epoch 150/200

Epoch 00150: val_loss did not improve from 0.42317
 - 3s - loss: 0.4386 - acc: 0.8356 - val_loss: 0.5387 - val_acc: 0.7890
Epoch 151/200

Epoch 00151: val_loss did not improve from 0.42317
 - 3s - loss: 0.4330 - acc: 0.8335 - val_loss: 0.4522 - val_acc: 0.8536
Epoch 152/200

Epoch 00152: val_loss did not improve from 0.42317
 - 3s - loss: 0.4433 - acc: 0.8343 - val_loss: 0.5034 - val_acc: 0.8313
Epoch 153/200

Epoch 00153: val_loss did not improve from 0.42317
 - 3s - loss: 0.4294 - acc: 0.8382 - val_loss: 0.5217 - val_acc: 0.8138
Epoch 154/200

Epoch 00154: val_loss did not improve from 0.42317
 - 3s - loss: 0.4410 - acc: 0.8337 - val_loss: 0.5512 - val_acc: 0.7407
Epoch 155/200

Epoch 00155: val_loss did not improve from 0.42317
 - 3s - loss: 0.4444 - acc: 0.8275 - val_loss: 0.5210 - val_acc: 0.8284
Epoch 156/200

Epoch 00156: val_loss did not improve from 0.42317
 - 3s - loss: 0.4459 - acc: 0.8266 - val_loss: 0.5073 - val_acc: 0.8140
Epoch 157/200

Epoch 00157: val_loss did not improve from 0.42317
 - 3s - loss: 0.4334 - acc: 0.8398 - val_loss: 0.4718 - val_acc: 0.8382
Epoch 158/200

Epoch 00158: val_loss did not improve from 0.42317
 - 3s - loss: 0.4383 - acc: 0.8337 - val_loss: 0.5072 - val_acc: 0.8269
Epoch 159/200

Epoch 00159: val_loss did not improve from 0.42317
 - 3s - loss: 0.4377 - acc: 0.8357 - val_loss: 0.4722 - val_acc: 0.8559
Epoch 160/200

Epoch 00160: val_loss did not improve from 0.42317
 - 3s - loss: 0.4423 - acc: 0.8430 - val_loss: 0.5563 - val_acc: 0.7686
Epoch 161/200

Epoch 00161: val_loss did not improve from 0.42317
 - 3s - loss: 0.4421 - acc: 0.8273 - val_loss: 0.5423 - val_acc: 0.7945
Epoch 162/200

Epoch 00162: val_loss did not improve from 0.42317
 - 3s - loss: 0.4459 - acc: 0.8446 - val_loss: 0.5905 - val_acc: 0.7391
Epoch 163/200

Epoch 00163: val_loss did not improve from 0.42317
 - 3s - loss: 0.4421 - acc: 0.8304 - val_loss: 0.5027 - val_acc: 0.8242
Epoch 164/200

Epoch 00164: val_loss did not improve from 0.42317
 - 3s - loss: 0.4360 - acc: 0.8324 - val_loss: 0.5449 - val_acc: 0.7813
Epoch 165/200

Epoch 00165: val_loss did not improve from 0.42317
 - 3s - loss: 0.4324 - acc: 0.8277 - val_loss: 0.5043 - val_acc: 0.8257
Epoch 166/200

Epoch 00166: val_loss did not improve from 0.42317
 - 3s - loss: 0.4477 - acc: 0.8396 - val_loss: 0.5635 - val_acc: 0.7464
Epoch 167/200

Epoch 00167: val_loss did not improve from 0.42317
 - 3s - loss: 0.4513 - acc: 0.8241 - val_loss: 0.5100 - val_acc: 0.8090
Epoch 168/200

Epoch 00168: val_loss did not improve from 0.42317
 - 3s - loss: 0.4405 - acc: 0.8391 - val_loss: 0.4943 - val_acc: 0.8399
Epoch 169/200

Epoch 00169: val_loss did not improve from 0.42317
 - 3s - loss: 0.4259 - acc: 0.8381 - val_loss: 0.5187 - val_acc: 0.8188
Epoch 170/200

Epoch 00170: val_loss did not improve from 0.42317
 - 3s - loss: 0.4525 - acc: 0.8340 - val_loss: 0.5166 - val_acc: 0.8126
Epoch 171/200

Epoch 00171: val_loss did not improve from 0.42317
 - 3s - loss: 0.4450 - acc: 0.8296 - val_loss: 0.4965 - val_acc: 0.8359
Epoch 172/200

Epoch 00172: val_loss did not improve from 0.42317
 - 3s - loss: 0.4516 - acc: 0.8272 - val_loss: 0.5426 - val_acc: 0.7893
Epoch 173/200

Epoch 00173: val_loss did not improve from 0.42317
 - 3s - loss: 0.4399 - acc: 0.8344 - val_loss: 0.4873 - val_acc: 0.8432
Epoch 174/200

Epoch 00174: val_loss did not improve from 0.42317
 - 3s - loss: 0.4475 - acc: 0.8227 - val_loss: 0.4826 - val_acc: 0.8282
Epoch 175/200

Epoch 00175: val_loss did not improve from 0.42317
 - 3s - loss: 0.4490 - acc: 0.8244 - val_loss: 0.5221 - val_acc: 0.8022
Epoch 176/200

Epoch 00176: val_loss did not improve from 0.42317
 - 3s - loss: 0.4458 - acc: 0.8288 - val_loss: 0.5257 - val_acc: 0.8163
Epoch 177/200

Epoch 00177: val_loss did not improve from 0.42317
 - 3s - loss: 0.4468 - acc: 0.8325 - val_loss: 0.4440 - val_acc: 0.8599
Epoch 178/200

Epoch 00178: val_loss did not improve from 0.42317
 - 3s - loss: 0.4461 - acc: 0.8397 - val_loss: 0.5470 - val_acc: 0.7720
Epoch 179/200

Epoch 00179: val_loss did not improve from 0.42317
 - 3s - loss: 0.4370 - acc: 0.8175 - val_loss: 0.4805 - val_acc: 0.8480
Epoch 180/200

Epoch 00180: val_loss did not improve from 0.42317
 - 3s - loss: 0.4418 - acc: 0.8357 - val_loss: 0.5854 - val_acc: 0.7187
Epoch 181/200

Epoch 00181: val_loss did not improve from 0.42317
 - 3s - loss: 0.4519 - acc: 0.8300 - val_loss: 0.4716 - val_acc: 0.8313
Epoch 182/200

Epoch 00182: val_loss did not improve from 0.42317
 - 3s - loss: 0.4332 - acc: 0.8379 - val_loss: 0.4946 - val_acc: 0.8238
Epoch 183/200

Epoch 00183: val_loss did not improve from 0.42317
 - 3s - loss: 0.4599 - acc: 0.8340 - val_loss: 0.5802 - val_acc: 0.7328
Epoch 184/200

Epoch 00184: val_loss did not improve from 0.42317
 - 3s - loss: 0.4493 - acc: 0.8237 - val_loss: 0.4818 - val_acc: 0.8576
Epoch 185/200

Epoch 00185: val_loss did not improve from 0.42317
 - 3s - loss: 0.4302 - acc: 0.8258 - val_loss: 0.4330 - val_acc: 0.8584
Epoch 186/200

Epoch 00186: val_loss did not improve from 0.42317
 - 3s - loss: 0.4577 - acc: 0.8327 - val_loss: 0.5157 - val_acc: 0.8092
Epoch 187/200

Epoch 00187: val_loss did not improve from 0.42317
 - 3s - loss: 0.4326 - acc: 0.8347 - val_loss: 0.5061 - val_acc: 0.8263
Epoch 188/200

Epoch 00188: val_loss did not improve from 0.42317
 - 3s - loss: 0.4357 - acc: 0.8338 - val_loss: 0.4996 - val_acc: 0.8219
Epoch 189/200

Epoch 00189: val_loss did not improve from 0.42317
 - 3s - loss: 0.4430 - acc: 0.8362 - val_loss: 0.4598 - val_acc: 0.8399
Epoch 190/200

Epoch 00190: val_loss did not improve from 0.42317
 - 3s - loss: 0.4485 - acc: 0.8315 - val_loss: 0.4877 - val_acc: 0.8355
Epoch 191/200

Epoch 00191: val_loss did not improve from 0.42317
 - 3s - loss: 0.4420 - acc: 0.8278 - val_loss: 0.5118 - val_acc: 0.8153
Epoch 192/200

Epoch 00192: val_loss did not improve from 0.42317
 - 3s - loss: 0.4474 - acc: 0.8296 - val_loss: 0.5254 - val_acc: 0.7990
Epoch 193/200

Epoch 00193: val_loss did not improve from 0.42317
 - 3s - loss: 0.4466 - acc: 0.8304 - val_loss: 0.5075 - val_acc: 0.8195
Epoch 194/200

Epoch 00194: val_loss did not improve from 0.42317
 - 3s - loss: 0.4364 - acc: 0.8324 - val_loss: 0.5333 - val_acc: 0.8186
Epoch 195/200

Epoch 00195: val_loss did not improve from 0.42317
 - 3s - loss: 0.4331 - acc: 0.8313 - val_loss: 0.5748 - val_acc: 0.7651
Epoch 196/200

Epoch 00196: val_loss did not improve from 0.42317
 - 3s - loss: 0.4524 - acc: 0.8335 - val_loss: 0.5233 - val_acc: 0.7936
Epoch 197/200

Epoch 00197: val_loss did not improve from 0.42317
 - 3s - loss: 0.4510 - acc: 0.8167 - val_loss: 0.4551 - val_acc: 0.8372
Epoch 198/200

Epoch 00198: val_loss did not improve from 0.42317
 - 3s - loss: 0.4425 - acc: 0.8306 - val_loss: 0.5123 - val_acc: 0.8099
Epoch 199/200

Epoch 00199: val_loss did not improve from 0.42317
 - 3s - loss: 0.4359 - acc: 0.8273 - val_loss: 0.6233 - val_acc: 0.6818
Epoch 200/200

Epoch 00200: val_loss did not improve from 0.42317
 - 3s - loss: 0.4282 - acc: 0.8337 - val_loss: 0.4790 - val_acc: 0.8453
/home/wtsmith/anaconda3/envs/arl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/wtsmith/anaconda3/envs/arl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/wtsmith/anaconda3/envs/arl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/wtsmith/anaconda3/envs/arl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/wtsmith/anaconda3/envs/arl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/wtsmith/anaconda3/envs/arl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])

Bad key "text.kerning_factor" on line 4 in
/home/wtsmith/anaconda3/envs/arl/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.
You probably need to get an updated matplotlibrc file from
http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template
or from the matplotlib source distribution
/home/wtsmith/anaconda3/envs/arl/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.
  FutureWarning)
Classification accuracy: 0.857054 
Classification accuracy: 0.949022 
Traceback (most recent call last):
  File "/home/wtsmith/anaconda3/envs/arl/lib/python3.6/site-packages/pandas/core/internals/managers.py", line 1662, in create_block_manager_from_blocks
    make_block(values=blocks[0], placement=slice(0, len(axes[0])))
  File "/home/wtsmith/anaconda3/envs/arl/lib/python3.6/site-packages/pandas/core/internals/blocks.py", line 2722, in make_block
    return klass(values, ndim=ndim, placement=placement)
  File "/home/wtsmith/anaconda3/envs/arl/lib/python3.6/site-packages/pandas/core/internals/blocks.py", line 131, in __init__
    f"Wrong number of items passed {len(self.values)}, "
ValueError: Wrong number of items passed 2, placement implies 4

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "MyERP.py", line 249, in <module>
    plot_confusion_matrix(preds, Y_test.argmax(axis = -1), names, title = 'EEGNet-8,2')
  File "/home/wtsmith/anaconda3/envs/arl/lib/python3.6/site-packages/pyriemann/utils/viz.py", line 17, in plot_confusion_matrix
    df = pd.DataFrame(data=cm, columns=target_names, index=target_names)
  File "/home/wtsmith/anaconda3/envs/arl/lib/python3.6/site-packages/pandas/core/frame.py", line 497, in __init__
    mgr = init_ndarray(data, index, columns, dtype=dtype, copy=copy)
  File "/home/wtsmith/anaconda3/envs/arl/lib/python3.6/site-packages/pandas/core/internals/construction.py", line 234, in init_ndarray
    return create_block_manager_from_blocks(block_values, [columns, index])
  File "/home/wtsmith/anaconda3/envs/arl/lib/python3.6/site-packages/pandas/core/internals/managers.py", line 1672, in create_block_manager_from_blocks
    raise construction_error(tot_items, blocks[0].shape[1:], axes, e)
ValueError: Shape of passed values is (2, 2), indices imply (4, 4)
2020-11-10 22:00:59.710923: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-11-10 22:00:59.828516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-10 22:00:59.829082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Quadro P5000 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:0b:00.0
totalMemory: 15.90GiB freeMemory: 15.36GiB
2020-11-10 22:00:59.829107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-11-10 22:01:00.142160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-10 22:01:00.142203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-11-10 22:01:00.142210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-11-10 22:01:00.142324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14877 MB memory) -> physical GPU (device: 0, name: Quadro P5000, pci bus id: 0000:0b:00.0, compute capability: 6.1)
class_weights {0: 7.528996865203761, 1: 0.5355669528375515}
X_train shape: (9607, 1, 32, 231)
X_testshape: (4806, 1, 32, 231)
9607 train samples
4806 test samples
chans: 32 samples: 231
Train on 9607 samples, validate on 4802 samples
Epoch 1/50

Epoch 00001: val_loss improved from inf to 0.24364, saving model to /tmp/checkpoint.h5
 - 37s - loss: 0.2567 - acc: 0.9327 - val_loss: 0.2436 - val_acc: 0.9336
Epoch 2/50

Epoch 00002: val_loss did not improve from 0.24364
 - 36s - loss: 0.2508 - acc: 0.9336 - val_loss: 0.2485 - val_acc: 0.9336
Epoch 3/50

Epoch 00003: val_loss improved from 0.24364 to 0.24232, saving model to /tmp/checkpoint.h5
 - 36s - loss: 0.2492 - acc: 0.9336 - val_loss: 0.2423 - val_acc: 0.9336
Epoch 4/50

Epoch 00004: val_loss did not improve from 0.24232
 - 36s - loss: 0.2474 - acc: 0.9336 - val_loss: 0.2438 - val_acc: 0.9336
Epoch 5/50

Epoch 00005: val_loss did not improve from 0.24232
 - 36s - loss: 0.2461 - acc: 0.9336 - val_loss: 0.2453 - val_acc: 0.9336
Epoch 6/50

Epoch 00006: val_loss did not improve from 0.24232
 - 36s - loss: 0.2435 - acc: 0.9336 - val_loss: 0.2437 - val_acc: 0.9336
Epoch 7/50

Epoch 00007: val_loss did not improve from 0.24232
 - 36s - loss: 0.2429 - acc: 0.9336 - val_loss: 0.2438 - val_acc: 0.9336
Epoch 8/50

Epoch 00008: val_loss did not improve from 0.24232
 - 36s - loss: 0.2422 - acc: 0.9336 - val_loss: 0.2436 - val_acc: 0.9336
Epoch 9/50

Epoch 00009: val_loss did not improve from 0.24232
 - 36s - loss: 0.2392 - acc: 0.9336 - val_loss: 0.2438 - val_acc: 0.9336
Epoch 10/50

Epoch 00010: val_loss did not improve from 0.24232
 - 36s - loss: 0.2375 - acc: 0.9336 - val_loss: 0.2446 - val_acc: 0.9336
Epoch 11/50

Epoch 00011: val_loss did not improve from 0.24232
 - 36s - loss: 0.2376 - acc: 0.9336 - val_loss: 0.2450 - val_acc: 0.9336
Epoch 12/50

Epoch 00012: val_loss did not improve from 0.24232
 - 36s - loss: 0.2352 - acc: 0.9336 - val_loss: 0.2449 - val_acc: 0.9334
Epoch 13/50

Epoch 00013: val_loss did not improve from 0.24232
 - 36s - loss: 0.2344 - acc: 0.9336 - val_loss: 0.2447 - val_acc: 0.9336
Epoch 14/50

Epoch 00014: val_loss did not improve from 0.24232
 - 36s - loss: 0.2319 - acc: 0.9336 - val_loss: 0.2442 - val_acc: 0.9336
Epoch 15/50

Epoch 00015: val_loss did not improve from 0.24232
 - 36s - loss: 0.2310 - acc: 0.9336 - val_loss: 0.2487 - val_acc: 0.9332
Epoch 16/50

Epoch 00016: val_loss did not improve from 0.24232
 - 36s - loss: 0.2298 - acc: 0.9337 - val_loss: 0.2479 - val_acc: 0.9332
Epoch 17/50

Epoch 00017: val_loss did not improve from 0.24232
 - 36s - loss: 0.2284 - acc: 0.9336 - val_loss: 0.2446 - val_acc: 0.9334
Epoch 18/50

Epoch 00018: val_loss did not improve from 0.24232
 - 36s - loss: 0.2273 - acc: 0.9337 - val_loss: 0.2479 - val_acc: 0.9332
Epoch 19/50

Epoch 00019: val_loss did not improve from 0.24232
 - 36s - loss: 0.2271 - acc: 0.9336 - val_loss: 0.2436 - val_acc: 0.9336
Epoch 20/50

Epoch 00020: val_loss did not improve from 0.24232
 - 36s - loss: 0.2267 - acc: 0.9335 - val_loss: 0.2462 - val_acc: 0.9336
Epoch 21/50

Epoch 00021: val_loss did not improve from 0.24232
 - 36s - loss: 0.2250 - acc: 0.9337 - val_loss: 0.2474 - val_acc: 0.9336
Epoch 22/50

Epoch 00022: val_loss did not improve from 0.24232
 - 36s - loss: 0.2237 - acc: 0.9336 - val_loss: 0.2464 - val_acc: 0.9334
Epoch 23/50

Epoch 00023: val_loss did not improve from 0.24232
 - 36s - loss: 0.2219 - acc: 0.9337 - val_loss: 0.2528 - val_acc: 0.9329
Epoch 24/50

Epoch 00024: val_loss did not improve from 0.24232
 - 36s - loss: 0.2204 - acc: 0.9337 - val_loss: 0.2496 - val_acc: 0.9329
Epoch 25/50

Epoch 00025: val_loss did not improve from 0.24232
 - 36s - loss: 0.2199 - acc: 0.9336 - val_loss: 0.2448 - val_acc: 0.9327
Epoch 26/50

Epoch 00026: val_loss did not improve from 0.24232
 - 36s - loss: 0.2203 - acc: 0.9339 - val_loss: 0.2468 - val_acc: 0.9332
Epoch 27/50

Epoch 00027: val_loss did not improve from 0.24232
 - 36s - loss: 0.2205 - acc: 0.9336 - val_loss: 0.2548 - val_acc: 0.9275
Epoch 28/50

Epoch 00028: val_loss did not improve from 0.24232
 - 36s - loss: 0.2203 - acc: 0.9335 - val_loss: 0.2478 - val_acc: 0.9329
Epoch 29/50

Epoch 00029: val_loss did not improve from 0.24232
 - 36s - loss: 0.2192 - acc: 0.9336 - val_loss: 0.2500 - val_acc: 0.9317
Epoch 30/50

Epoch 00030: val_loss did not improve from 0.24232
 - 36s - loss: 0.2192 - acc: 0.9336 - val_loss: 0.2504 - val_acc: 0.9296
Epoch 31/50

Epoch 00031: val_loss did not improve from 0.24232
 - 36s - loss: 0.2210 - acc: 0.9332 - val_loss: 0.2598 - val_acc: 0.9273
Epoch 32/50

Epoch 00032: val_loss did not improve from 0.24232
 - 36s - loss: 0.2182 - acc: 0.9337 - val_loss: 0.2485 - val_acc: 0.9332
Epoch 33/50

Epoch 00033: val_loss did not improve from 0.24232
 - 36s - loss: 0.2163 - acc: 0.9338 - val_loss: 0.2459 - val_acc: 0.9334
Epoch 34/50

Epoch 00034: val_loss did not improve from 0.24232
 - 36s - loss: 0.2184 - acc: 0.9336 - val_loss: 0.2461 - val_acc: 0.9336
Epoch 35/50

Epoch 00035: val_loss did not improve from 0.24232
 - 36s - loss: 0.2157 - acc: 0.9338 - val_loss: 0.2547 - val_acc: 0.9317
Epoch 36/50

Epoch 00036: val_loss did not improve from 0.24232
 - 36s - loss: 0.2149 - acc: 0.9340 - val_loss: 0.2613 - val_acc: 0.9309
Epoch 37/50

Epoch 00037: val_loss did not improve from 0.24232
 - 36s - loss: 0.2104 - acc: 0.9337 - val_loss: 0.2447 - val_acc: 0.9336
Epoch 38/50

Epoch 00038: val_loss did not improve from 0.24232
 - 36s - loss: 0.2117 - acc: 0.9338 - val_loss: 0.2548 - val_acc: 0.9327
Epoch 39/50

Epoch 00039: val_loss did not improve from 0.24232
 - 36s - loss: 0.2122 - acc: 0.9333 - val_loss: 0.2542 - val_acc: 0.9327
Epoch 40/50

Epoch 00040: val_loss did not improve from 0.24232
 - 36s - loss: 0.2098 - acc: 0.9341 - val_loss: 0.2583 - val_acc: 0.9321
Epoch 41/50

Epoch 00041: val_loss did not improve from 0.24232
 - 36s - loss: 0.2095 - acc: 0.9334 - val_loss: 0.2581 - val_acc: 0.9311
Epoch 42/50

Epoch 00042: val_loss did not improve from 0.24232
 - 36s - loss: 0.2107 - acc: 0.9334 - val_loss: 0.2558 - val_acc: 0.9325
Epoch 43/50

Epoch 00043: val_loss did not improve from 0.24232
 - 36s - loss: 0.2096 - acc: 0.9333 - val_loss: 0.2500 - val_acc: 0.9332
Epoch 44/50

Epoch 00044: val_loss did not improve from 0.24232
 - 36s - loss: 0.2074 - acc: 0.9334 - val_loss: 0.2576 - val_acc: 0.9321
Epoch 45/50

Epoch 00045: val_loss did not improve from 0.24232
 - 36s - loss: 0.2096 - acc: 0.9334 - val_loss: 0.2866 - val_acc: 0.9211
Epoch 46/50

Epoch 00046: val_loss did not improve from 0.24232
 - 36s - loss: 0.2063 - acc: 0.9340 - val_loss: 0.2630 - val_acc: 0.9296
Epoch 47/50

Epoch 00047: val_loss did not improve from 0.24232
 - 36s - loss: 0.2068 - acc: 0.9338 - val_loss: 0.2585 - val_acc: 0.9304
Epoch 48/50

Epoch 00048: val_loss did not improve from 0.24232
 - 36s - loss: 0.2064 - acc: 0.9339 - val_loss: 0.2552 - val_acc: 0.9334
Epoch 49/50

Epoch 00049: val_loss did not improve from 0.24232
 - 36s - loss: 0.2104 - acc: 0.9336 - val_loss: 0.2541 - val_acc: 0.9336
Epoch 50/50

Epoch 00050: val_loss did not improve from 0.24232
 - 36s - loss: 0.2084 - acc: 0.9341 - val_loss: 0.2637 - val_acc: 0.9288
/home/wtsmith/anaconda3/envs/arl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/wtsmith/anaconda3/envs/arl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/wtsmith/anaconda3/envs/arl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/wtsmith/anaconda3/envs/arl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/wtsmith/anaconda3/envs/arl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/wtsmith/anaconda3/envs/arl/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])

Bad key "text.kerning_factor" on line 4 in
/home/wtsmith/anaconda3/envs/arl/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.
You probably need to get an updated matplotlibrc file from
http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template
or from the matplotlib source distribution
/home/wtsmith/anaconda3/envs/arl/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.
  FutureWarning)
Classification accuracy: 0.943820 
Y_test [[0. 1.]
 [0. 1.]
 [0. 1.]
 ...
 [0. 1.]
 [0. 1.]
 [0. 1.]]
y_test [2 2 2 ... 2 2 2]
Y_test.argmax(axis=-1) [1 1 1 ... 1 1 1]
preds [1 1 1 ... 1 1 1]
roc_auc_score 0.5164317786412072
Classification accuracy: 0.949022 
